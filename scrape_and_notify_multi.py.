"""
Scraper multi‑fuente para Oussama.
Lee `sources.yml`, descarga los listados y (por ahora) solo muestra cuántos
artículos se han parseado por fuente. En el siguiente paso añadiremos el envío
a Telegram y la lógica de deduplicación.
"""

import yaml
import re
from urllib.parse import urljoin
from bs4 import BeautifulSoup

# Reutilizamos utilidades del scraper original
from scrape_and_notify import (
    fetch_url,
    send_article,   # se usará en el paso siguiente
    load_sent,
    save_sent,
)

# === Cargar configuración de fuentes ===
with open("sources.yml", "r", encoding="utf-8") as f:
    SOURCES = yaml.safe_load(f)


def parse_articles_generic(html: str, cfg: dict) -> list[dict]:
    """
    Extrae los artículos de `html` usando los selectores que vienen en `cfg`.

    Devuelve una lista de dicts con:
      - headline
      - description
      - link
      - image_url
      - date        (texto original)
      - parsed_date (aaaa-mm-dd o "", si no se puede convertir)
    """
    soup = BeautifulSoup(html, "html.parser")
    sel = cfg["selectors"]

    articles = []
    for block in soup.select(sel["container"]):
        # --- titular y enlace -------------------------------------------------
        a_tag = block.select_one(sel["headline"])
        if not a_tag:
            continue

        headline = a_tag.get_text(strip=True)
        link_raw = a_tag.get(sel.get("link_attr", "href"), "")
        if not link_raw:
            continue
        link = urljoin(cfg["base_url"], link_raw)

        # --- descripción / snippet -------------------------------------------
        description = ""
        desc_sel = sel.get("description")
        if desc_sel:
            desc_tag = block.select_one(desc_sel)
            if desc_tag:
                description = desc_tag.get_text(strip=True)

        # --- imagen -----------------------------------------------------------
        image_url = ""
        img_sel = sel.get("image")
        if img_sel:
            if "::attr(" in img_sel:                             # formato css::attr(src)
                css, attr = re.match(r"(.+)::attr\((.+)\)", img_sel).groups()
                img_tag = block.select_one(css)
                if img_tag and img_tag.has_attr(attr):
                    image_url = urljoin(cfg["base_url"], img_tag[attr])
            else:                                               # formato clásico "img"
                img_tag = block.select_one(img_sel)
                if img_tag and img_tag.has_attr("src"):
                    image_url = urljoin(cfg["base_url"], img_tag["src"])

        # --- fecha ------------------------------------------------------------
        date_text = ""
        date_sel = sel.get("date")
        if date_sel:
            date_tag = block.select_one(date_sel)
            if date_tag:
                date_text = date_tag.get_text(strip=True)

        parsed_date = ""
        if cfg.get("date_regex"):
            m = re.search(cfg["date_regex"], date_text)
            if m:
                # Formato con nombre de mes en francés (usa month_map)
                if cfg.get("month_map"):
                    day, mon_name, year = m.groups()
                    mon_num = cfg["month_map"].get(mon_name, "")
                    if mon_num:
                        parsed_date = f"{year}-{mon_num}-{int(day):02d}"
                else:
                    # dd/mm/aaaa  ->  aaaa-mm-dd
                    # aaaa-mm-dd  ->  aaaa-mm-dd
                    g1, g2, g3 = m.groups()
                    if "/" in date_text:       # dd/mm/aaaa
                        d, mth, y = g1, g2, g3
                        parsed_date = f"{y}-{int(mth):02d}-{int(d):02d}"
                    else:                      # aaaa-mm-dd
                        y, mth, d = g1, g2, g3
                        parsed_date = f"{y}-{mth}-{d}"

        articles.append(
            {
                "headline": headline,
                "description": description,
                "link": link,
                "image_url": image_url,
                "date": date_text,
                "parsed_date": parsed_date,
            }
        )

    return articles


def main() -> None:
    sent_urls = load_sent()  # aún sin usar; lo integraremos después

    for src in SOURCES:
        print(f"[DEBUG] Fetching {src['name']} …")
        html = fetch_url(src["list_url"])
        articles = parse_articles_generic(html, src)
        print(f"[DEBUG] {src['name']}: {len(articles)} artículos parseados")
        # En el siguiente paso filtraremos y enviaremos a Telegram


if __name__ == "__main__":
    main()
